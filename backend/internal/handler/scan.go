package handler

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"strconv"
	"time"

	"embyforge/internal/emby"
	"embyforge/internal/model"
	"embyforge/internal/service"
	"embyforge/internal/tmdb"

	"github.com/gin-gonic/gin"
	"gorm.io/gorm"
)

// defaultScanTimeout æ‰«ææ“ä½œé»˜è®¤è¶…æ—¶æ—¶é—´
const defaultScanTimeout = 30 * time.Minute

// ScanHandler æ‰«æå¤„ç†å™¨
type ScanHandler struct {
	DB          *gorm.DB
	ScanService *service.ScanService
}

// NewScanHandler åˆ›å»ºæ‰«æå¤„ç†å™¨
func NewScanHandler(db *gorm.DB) *ScanHandler {
	return &ScanHandler{
		DB:          db,
		ScanService: service.NewScanService(db),
	}
}

// getTMDBAPIKey ä»æ•°æ®åº“è¯»å– TMDB API Key
func (h *ScanHandler) getTMDBAPIKey() (string, error) {
	var config model.SystemConfig
	if err := h.DB.Where("key = ?", "tmdb_api_key").First(&config).Error; err != nil {
		return "", err
	}
	return config.Value, nil
}

// getEmbyClient ä»æ•°æ®åº“è·å– Emby é…ç½®å¹¶åˆ›å»ºå®¢æˆ·ç«¯
func (h *ScanHandler) getEmbyClient() (*emby.Client, error) {
	var config model.EmbyConfig
	if err := h.DB.First(&config).Error; err != nil {
		return nil, err
	}
	return emby.NewClient(config.Host, config.Port, config.APIKey), nil
}

// StartScrapeAnomalyScan å¯åŠ¨åˆ®å‰Šå¼‚å¸¸æ‰«æ
func (h *ScanHandler) StartScrapeAnomalyScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åˆ®å‰Šå¼‚å¸¸æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanScrapeAnomalies(client)
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// StartDuplicateMediaScan å¯åŠ¨é‡å¤åª’ä½“æ‰«æ
func (h *ScanHandler) StartDuplicateMediaScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹é‡å¤åª’ä½“æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanDuplicateMedia(client)
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// GetDuplicateMedia åˆ†é¡µè·å–é‡å¤åª’ä½“ç»“æœï¼ˆæŒ‰ GroupKey åˆ†ç»„ï¼‰
func (h *ScanHandler) GetDuplicateMedia(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// è·å–ä¸åŒåˆ†ç»„çš„æ€»æ•°
	var totalGroups int64
	h.DB.Model(&model.DuplicateMedia{}).Distinct("group_key").Count(&totalGroups)

	// åˆ†é¡µè·å–åˆ†ç»„é”®å’Œåˆ†ç»„å
	type groupInfo struct {
		GroupKey  string `json:"group_key"`
		GroupName string `json:"group_name"`
		Count     int64  `json:"count"`
	}
	var groups []groupInfo
	offset := (page - 1) * pageSize
	h.DB.Model(&model.DuplicateMedia{}).
		Select("group_key, MAX(group_name) as group_name, COUNT(*) as count").
		Group("group_key").
		Order("count DESC, group_key ASC").
		Offset(offset).
		Limit(pageSize).
		Find(&groups)

	// è·å–è¿™äº›åˆ†ç»„ä¸‹çš„æ‰€æœ‰è®°å½•
	groupKeys := make([]string, len(groups))
	for i, g := range groups {
		groupKeys[i] = g.GroupKey
	}

	var duplicates []model.DuplicateMedia
	if len(groupKeys) > 0 {
		h.DB.Where("group_key IN ?", groupKeys).Order("group_key ASC, type ASC, name ASC").Find(&duplicates)
	}

	// æŒ‰ GroupKey åˆ†ç»„è¿”å›ï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯
	type groupResult struct {
		GroupKey  string                 `json:"group_key"`
		GroupName string                 `json:"group_name"`
		Count     int64                  `json:"count"`
		Items     []model.DuplicateMedia `json:"items"`
	}

	// æ„å»ºåˆ†ç»„æ˜ å°„
	itemsByKey := make(map[string][]model.DuplicateMedia)
	for _, d := range duplicates {
		itemsByKey[d.GroupKey] = append(itemsByKey[d.GroupKey], d)
	}

	var results []groupResult
	for _, g := range groups {
		results = append(results, groupResult{
			GroupKey:  g.GroupKey,
			GroupName: g.GroupName,
			Count:     g.Count,
			Items:     itemsByKey[g.GroupKey],
		})
	}

	c.JSON(http.StatusOK, gin.H{
		"data":         results,
		"total_groups": totalGroups,
		"page":         page,
		"page_size":    pageSize,
	})
}

// GetScrapeAnomalies åˆ†é¡µè·å–åˆ®å‰Šå¼‚å¸¸ç»“æœ
func (h *ScanHandler) GetScrapeAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	var total int64
	h.DB.Model(&model.ScrapeAnomaly{}).Count(&total)

	var anomalies []model.ScrapeAnomaly
	offset := (page - 1) * pageSize
	h.DB.Offset(offset).Limit(pageSize).Order("id ASC").Find(&anomalies)

	c.JSON(http.StatusOK, gin.H{
		"data":      anomalies,
		"total":     total,
		"page":      page,
		"page_size": pageSize,
	})
}

// StartEpisodeMappingScan å¯åŠ¨å¼‚å¸¸æ˜ å°„æ‰«æ
func (h *ScanHandler) StartEpisodeMappingScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹å¼‚å¸¸æ˜ å°„æ‰«æ...")
	embyClient, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.ScanEpisodeMappingWithContext(ctx, embyClient, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "æ‰«ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("å¼‚å¸¸æ˜ å°„", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeScrapeAnomalies POST /api/analyze/scrape-anomaly - åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸
func (h *ScanHandler) AnalyzeScrapeAnomalies(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeScrapeAnomaliesFromCache()
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeDuplicateMedia POST /api/analyze/duplicate-media - åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“
func (h *ScanHandler) AnalyzeDuplicateMedia(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeDuplicateMediaFromCache()
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeEpisodeMapping POST /api/analyze/episode-mapping - åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„
func (h *ScanHandler) AnalyzeEpisodeMapping(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	// è·å– TMDB API Key
	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.AnalyzeEpisodeMappingFromCacheWithContext(ctx, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "åˆ†ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	// æŸ¥è¯¢å»é‡åçš„å¼‚å¸¸èŠ‚ç›®æ•°ï¼ˆä¸ç»Ÿè®¡å¡ç‰‡ä¿æŒä¸€è‡´ï¼‰
	var distinctCount int64
	h.DB.Model(&model.EpisodeMappingAnomaly{}).Distinct("emby_item_id").Count(&distinctCount)

	log.Printf("âœ… å¼‚å¸¸æ˜ å°„åˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		result.TotalScanned, distinctCount, result.ErrorCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
		"anomaly_show_count": distinctCount,
	})
}

// CleanupDuplicateMedia POST /api/cleanup/duplicate-media - æ‰¹é‡æ¸…ç†é‡å¤åª’ä½“
// æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å¾…åˆ é™¤ emby_item_id åˆ—è¡¨ï¼Œé€ä¸ªè°ƒç”¨ Emby DeleteVersion æ¥å£
func (h *ScanHandler) CleanupDuplicateMedia(c *gin.Context) {
	var req struct {
		Items []string `json:"items"` // è¦åˆ é™¤çš„ emby_item_id åˆ—è¡¨
	}
	if err := c.ShouldBindJSON(&req); err != nil || len(req.Items) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¡ç›®",
		})
		return
	}

	log.Printf("ğŸ§¹ å¼€å§‹æ‰¹é‡æ¸…ç†é‡å¤åª’ä½“ï¼Œå…± %d ä¸ªæ¡ç›®...", len(req.Items))

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Minute)
	defer cancel()

	deletedCount := 0
	failedCount := 0
	var freedSize int64
	var failedItems []string

	// æŸ¥è¯¢è¿™äº›æ¡ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—å’Œç»Ÿè®¡é‡Šæ”¾ç©ºé—´ï¼‰
	var toDelete []model.DuplicateMedia
	h.DB.Where("emby_item_id IN ?", req.Items).Find(&toDelete)

	// æ„å»º emby_item_id -> DuplicateMedia æ˜ å°„
	itemMap := make(map[string]model.DuplicateMedia)
	for _, d := range toDelete {
		itemMap[d.EmbyItemID] = d
	}

	for _, embyID := range req.Items {
		item, exists := itemMap[embyID]

		// è°ƒç”¨ Emby DeleteVersion æ¥å£
		if err := client.DeleteVersion(ctx, embyID); err != nil {
			log.Printf("âŒ åˆ é™¤ç‰ˆæœ¬å¤±è´¥ [%s]: %v", embyID, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		if exists {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s] %s (%.1f MB)", embyID, item.Path, float64(item.FileSize)/1024/1024)
			freedSize += item.FileSize
		} else {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s]", embyID)
		}
		deletedCount++
	}

	// ä»æ•°æ®åº“ä¸­åˆ é™¤å·²æ¸…ç†çš„è®°å½•
	if deletedCount > 0 {
		// æ’é™¤å¤±è´¥çš„ï¼Œåªåˆ é™¤æˆåŠŸçš„
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.DuplicateMedia{})
		}

		// æ¸…ç†åªå‰©ä¸€æ¡è®°å½•çš„åˆ†ç»„ï¼ˆä¸å†æ˜¯é‡å¤ï¼‰
		h.DB.Exec(`DELETE FROM duplicate_media WHERE group_key IN (
			SELECT group_key FROM duplicate_media GROUP BY group_key HAVING COUNT(*) < 2
		)`)
	}

	log.Printf("âœ… é‡å¤åª’ä½“æ¸…ç†å®Œæˆ: åˆ é™¤ %d ä¸ª, é‡Šæ”¾ %.1f MB, å¤±è´¥ %d ä¸ª",
		deletedCount, float64(freedSize)/1024/1024, failedCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "æ¸…ç†å®Œæˆ",
		"data": gin.H{
			"deleted_count": deletedCount,
			"freed_size":    freedSize,
			"failed_count":  failedCount,
			"failed_items":  failedItems,
		},
	})
}

// PreviewDuplicateCleanup GET /api/cleanup/duplicate-media/preview - é¢„è§ˆå¾…æ¸…ç†çš„é‡å¤åª’ä½“
// è¿”å›æ‰€æœ‰é‡å¤ç»„ï¼Œæ¯ç»„åŒ…å«å…¨éƒ¨æ¡ç›®ï¼Œå¹¶æ ‡è®°å»ºè®®åˆ é™¤çš„ï¼ˆä½“ç§¯è¾ƒå°çš„ï¼‰
func (h *ScanHandler) PreviewDuplicateCleanup(c *gin.Context) {
	// è·å–æ‰€æœ‰é‡å¤åª’ä½“è®°å½•ï¼ŒæŒ‰åˆ†ç»„å’Œæ–‡ä»¶å¤§å°å‡åºæ’åº
	var duplicates []model.DuplicateMedia
	h.DB.Order("group_key ASC, file_size ASC").Find(&duplicates)

	// æŒ‰ group_key åˆ†ç»„
	groups := make(map[string][]model.DuplicateMedia)
	var groupOrder []string
	for _, d := range duplicates {
		if _, exists := groups[d.GroupKey]; !exists {
			groupOrder = append(groupOrder, d.GroupKey)
		}
		groups[d.GroupKey] = append(groups[d.GroupKey], d)
	}

	type previewItem struct {
		EmbyItemID    string `json:"emby_item_id"`
		Name          string `json:"name"`
		Type          string `json:"type"`
		Path          string `json:"path"`
		FileSize      int64  `json:"file_size"`
		ShouldDelete  bool   `json:"should_delete"` // å»ºè®®åˆ é™¤ï¼ˆä½“ç§¯è¾ƒå°çš„ï¼‰
	}

	type previewGroup struct {
		GroupKey  string        `json:"group_key"`
		GroupName string        `json:"group_name"`
		Items     []previewItem `json:"items"`
	}

	var result []previewGroup
	totalDeleteCount := 0
	var totalFreedSize int64

	for _, key := range groupOrder {
		groupItems := groups[key]
		if len(groupItems) < 2 {
			continue
		}

		pg := previewGroup{
			GroupKey:  key,
			GroupName: groupItems[0].GroupName,
		}

		// æŒ‰æ–‡ä»¶å¤§å°å‡åºæ’åˆ—ï¼Œä¿ç•™æœ€åä¸€ä¸ªï¼ˆä½“ç§¯æœ€å¤§çš„ï¼‰ï¼Œå…¶ä½™å»ºè®®åˆ é™¤
		// å¤§å°ç›¸åŒæ—¶é»˜è®¤åˆ é™¤æ’åœ¨å‰é¢çš„
		lastIdx := len(groupItems) - 1
		for i, item := range groupItems {
			shouldDelete := i < lastIdx // æœ€åä¸€ä¸ªä¿ç•™ï¼Œå…¶ä½™åˆ é™¤
			pg.Items = append(pg.Items, previewItem{
				EmbyItemID:   item.EmbyItemID,
				Name:         item.Name,
				Type:         item.Type,
				Path:         item.Path,
				FileSize:     item.FileSize,
				ShouldDelete: shouldDelete,
			})
			if shouldDelete {
				totalDeleteCount++
				totalFreedSize += item.FileSize
			}
		}

		result = append(result, pg)
	}

	c.JSON(http.StatusOK, gin.H{
		"data":               result,
		"total_groups":       len(result),
		"total_delete_count": totalDeleteCount,
		"total_freed_size":   totalFreedSize,
	})
}

// FormatAnalysisSummary æ ¼å¼åŒ–åˆ†æç»“æœæ‘˜è¦æ—¥å¿—å­—ç¬¦ä¸²
func FormatAnalysisSummary(analysisType string, result *service.ScanResult) string {
	return fmt.Sprintf("âœ… %såˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		analysisType, result.TotalScanned, result.AnomalyCount, result.ErrorCount)
}

// CleanupScrapeAnomalies POST /api/cleanup/scrape-anomaly - æ‰¹é‡åˆ é™¤åˆ®å‰Šå¼‚å¸¸æ¡ç›®
// æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å¾…åˆ é™¤ emby_item_id åˆ—è¡¨ï¼Œé€ä¸ªè°ƒç”¨ Emby DeleteItem æ¥å£
func (h *ScanHandler) CleanupScrapeAnomalies(c *gin.Context) {
	var req struct {
		Items []string `json:"items"` // è¦åˆ é™¤çš„ emby_item_id åˆ—è¡¨
	}
	if err := c.ShouldBindJSON(&req); err != nil || len(req.Items) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¡ç›®",
		})
		return
	}

	log.Printf("ğŸ§¹ å¼€å§‹æ‰¹é‡åˆ é™¤åˆ®å‰Šå¼‚å¸¸æ¡ç›®ï¼Œå…± %d ä¸ª...", len(req.Items))

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Minute)
	defer cancel()

	deletedCount := 0
	failedCount := 0
	var failedItems []string

	// æŸ¥è¯¢è¿™äº›æ¡ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
	var toDelete []model.ScrapeAnomaly
	h.DB.Where("emby_item_id IN ?", req.Items).Find(&toDelete)

	// æ„å»º emby_item_id -> ScrapeAnomaly æ˜ å°„
	itemMap := make(map[string]model.ScrapeAnomaly)
	for _, d := range toDelete {
		itemMap[d.EmbyItemID] = d
	}

	for _, embyID := range req.Items {
		item, exists := itemMap[embyID]

		// è°ƒç”¨ Emby DeleteItem æ¥å£
		if err := client.DeleteItem(ctx, embyID); err != nil {
			log.Printf("âŒ åˆ é™¤æ¡ç›®å¤±è´¥ [%s]: %v", embyID, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		if exists {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s] %s", embyID, item.Name)
		} else {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s]", embyID)
		}
		deletedCount++
	}

	// ä»æ•°æ®åº“ä¸­åˆ é™¤å·²æ¸…ç†çš„è®°å½•
	if deletedCount > 0 {
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.ScrapeAnomaly{})
		}
	}

	// åŒæ—¶æ¸…ç† media_cache ä¸­å¯¹åº”çš„ç¼“å­˜è®°å½•
	if deletedCount > 0 {
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.MediaCache{})
		}
	}

	log.Printf("âœ… åˆ®å‰Šå¼‚å¸¸æ¸…ç†å®Œæˆ: åˆ é™¤ %d ä¸ª, å¤±è´¥ %d ä¸ª", deletedCount, failedCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "æ¸…ç†å®Œæˆ",
		"data": gin.H{
			"deleted_count": deletedCount,
			"failed_count":  failedCount,
			"failed_items":  failedItems,
		},
	})
}

// GetAnalysisStatus è·å–å„åˆ†ææ¨¡å—çš„æœ€ååˆ†ææ—¶é—´å’Œå¼‚å¸¸æ•°é‡
func (h *ScanHandler) GetAnalysisStatus(c *gin.Context) {
	type moduleStatus struct {
		LastAnalyzedAt *time.Time `json:"last_analyzed_at"`
		AnomalyCount   int64     `json:"anomaly_count"`
	}

	status := make(map[string]moduleStatus)

	modules := []struct {
		key   string
		model interface{}
		countDistinct string // å¦‚æœéœ€è¦ distinct è®¡æ•°
	}{
		{"scrape_anomaly", &model.ScrapeAnomaly{}, ""},
		{"duplicate_media", &model.DuplicateMedia{}, "group_key"},
		{"episode_mapping", &model.EpisodeMappingAnomaly{}, "emby_item_id"},
	}

	for _, m := range modules {
		var count int64
		if m.countDistinct != "" {
			h.DB.Model(m.model).Distinct(m.countDistinct).Count(&count)
		} else {
			h.DB.Model(m.model).Count(&count)
		}

		// ä» scan_logs è·å–æœ€åæ‰§è¡Œæ—¶é—´
		var lastLog model.ScanLog
		var lastTime *time.Time
		if err := h.DB.Where("module = ?", m.key).Order("finished_at DESC").First(&lastLog).Error; err == nil {
			lastTime = &lastLog.FinishedAt
		}

		status[m.key] = moduleStatus{LastAnalyzedAt: lastTime, AnomalyCount: count}
	}

	c.JSON(http.StatusOK, gin.H{
		"data": status,
	})
}

// EpisodeMappingGroup æŒ‰èŠ‚ç›®èšåˆçš„å¼‚å¸¸æ˜ å°„ç»„
type EpisodeMappingGroup struct {
	EmbyItemID string                        `json:"emby_item_id"`
	Name       string                        `json:"name"`
	TmdbID     int                           `json:"tmdb_id"`
	Seasons    []model.EpisodeMappingAnomaly `json:"seasons"`
}

// GetEpisodeMappingAnomalies åˆ†é¡µè·å–å¼‚å¸¸æ˜ å°„ç»“æœï¼ˆæŒ‰èŠ‚ç›®èšåˆï¼‰
func (h *ScanHandler) GetEpisodeMappingAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// æŒ‰ emby_item_id åˆ†ç»„è®¡ç®—æ€»èŠ‚ç›®æ•°
	var total int64
	h.DB.Model(&model.EpisodeMappingAnomaly{}).Distinct("emby_item_id").Count(&total)

	// åˆ†é¡µè·å–å½“å‰é¡µçš„èŠ‚ç›® ID åˆ—è¡¨ï¼ˆæŒ‰åç§°æ’åºï¼‰
	var embyItemIDs []string
	offset := (page - 1) * pageSize
	h.DB.Model(&model.EpisodeMappingAnomaly{}).
		Select("emby_item_id").
		Group("emby_item_id").
		Order("MIN(name) ASC").
		Offset(offset).Limit(pageSize).
		Pluck("emby_item_id", &embyItemIDs)

	var groups []EpisodeMappingGroup
	if len(embyItemIDs) > 0 {
		// è·å–è¿™äº›èŠ‚ç›®çš„æ‰€æœ‰å¼‚å¸¸å­£æ•°æ®ï¼ŒæŒ‰å­£å·æ’åº
		var anomalies []model.EpisodeMappingAnomaly
		h.DB.Where("emby_item_id IN ?", embyItemIDs).
			Order("season_number ASC").
			Find(&anomalies)

		// æŒ‰ emby_item_id èšåˆ
		groupMap := make(map[string]*EpisodeMappingGroup)
		groupOrder := make([]string, 0) // ä¿æŒé¡ºåº
		for _, a := range anomalies {
			g, exists := groupMap[a.EmbyItemID]
			if !exists {
				g = &EpisodeMappingGroup{
					EmbyItemID: a.EmbyItemID,
					Name:       a.Name,
					TmdbID:     a.TmdbID,
					Seasons:    []model.EpisodeMappingAnomaly{},
				}
				groupMap[a.EmbyItemID] = g
				groupOrder = append(groupOrder, a.EmbyItemID)
			}
			g.Seasons = append(g.Seasons, a)
		}

		// æŒ‰åŸå§‹é¡ºåºè¾“å‡º
		for _, id := range groupOrder {
			groups = append(groups, *groupMap[id])
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"data":      groups,
		"total":     total,
		"page":      page,
		"page_size": pageSize,
	})
}
