package handler

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"sort"
	"strconv"
	"time"

	"embyforge/internal/emby"
	"embyforge/internal/model"
	"embyforge/internal/service"
	"embyforge/internal/tmdb"

	"github.com/gin-gonic/gin"
	"gorm.io/gorm"
)

// defaultScanTimeout æ‰«ææ“ä½œé»˜è®¤è¶…æ—¶æ—¶é—´
const defaultScanTimeout = 30 * time.Minute

// ScanHandler æ‰«æå¤„ç†å™¨
type ScanHandler struct {
	DB          *gorm.DB
	ScanService *service.ScanService
}

// NewScanHandler åˆ›å»ºæ‰«æå¤„ç†å™¨
func NewScanHandler(db *gorm.DB) *ScanHandler {
	return &ScanHandler{
		DB:          db,
		ScanService: service.NewScanService(db),
	}
}

// getTMDBAPIKey ä»æ•°æ®åº“è¯»å– TMDB API Key
func (h *ScanHandler) getTMDBAPIKey() (string, error) {
	var config model.SystemConfig
	if err := h.DB.Where("key = ?", "tmdb_api_key").First(&config).Error; err != nil {
		return "", err
	}
	return config.Value, nil
}

// getEmbyClient ä»æ•°æ®åº“è·å– Emby é…ç½®å¹¶åˆ›å»ºå®¢æˆ·ç«¯
func (h *ScanHandler) getEmbyClient() (*emby.Client, error) {
	var config model.EmbyConfig
	if err := h.DB.First(&config).Error; err != nil {
		return nil, err
	}
	return emby.NewClient(config.Host, config.Port, config.APIKey), nil
}

// StartScrapeAnomalyScan å¯åŠ¨åˆ®å‰Šå¼‚å¸¸æ‰«æ
func (h *ScanHandler) StartScrapeAnomalyScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åˆ®å‰Šå¼‚å¸¸æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanScrapeAnomalies(client)
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// StartDuplicateMediaScan å¯åŠ¨é‡å¤åª’ä½“æ‰«æ
func (h *ScanHandler) StartDuplicateMediaScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹é‡å¤åª’ä½“æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanDuplicateMedia(client)
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// GetDuplicateMedia åˆ†é¡µè·å–é‡å¤åª’ä½“ç»“æœï¼ˆæŒ‰ GroupKey åˆ†ç»„ï¼‰
func (h *ScanHandler) GetDuplicateMedia(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// è·å–ä¸åŒåˆ†ç»„çš„æ€»æ•°
	var totalGroups int64
	h.DB.Model(&model.DuplicateMedia{}).Distinct("group_key").Count(&totalGroups)

	// åˆ†é¡µè·å–åˆ†ç»„é”®å’Œåˆ†ç»„å
	type groupInfo struct {
		GroupKey  string `json:"group_key"`
		GroupName string `json:"group_name"`
		Count     int64  `json:"count"`
	}
	var groups []groupInfo
	offset := (page - 1) * pageSize
	h.DB.Model(&model.DuplicateMedia{}).
		Select("group_key, MAX(group_name) as group_name, COUNT(*) as count").
		Group("group_key").
		Order("count DESC, group_key ASC").
		Offset(offset).
		Limit(pageSize).
		Find(&groups)

	// è·å–è¿™äº›åˆ†ç»„ä¸‹çš„æ‰€æœ‰è®°å½•
	groupKeys := make([]string, len(groups))
	for i, g := range groups {
		groupKeys[i] = g.GroupKey
	}

	var duplicates []model.DuplicateMedia
	if len(groupKeys) > 0 {
		h.DB.Where("group_key IN ?", groupKeys).Order("group_key ASC, type ASC, name ASC").Find(&duplicates)
	}

	// æŒ‰ GroupKey åˆ†ç»„è¿”å›ï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯
	type groupResult struct {
		GroupKey  string                 `json:"group_key"`
		GroupName string                 `json:"group_name"`
		Count     int64                  `json:"count"`
		Items     []model.DuplicateMedia `json:"items"`
	}

	// æ„å»ºåˆ†ç»„æ˜ å°„
	itemsByKey := make(map[string][]model.DuplicateMedia)
	for _, d := range duplicates {
		itemsByKey[d.GroupKey] = append(itemsByKey[d.GroupKey], d)
	}

	var results []groupResult
	for _, g := range groups {
		results = append(results, groupResult{
			GroupKey:  g.GroupKey,
			GroupName: g.GroupName,
			Count:     g.Count,
			Items:     itemsByKey[g.GroupKey],
		})
	}

	c.JSON(http.StatusOK, gin.H{
		"data":         results,
		"total_groups": totalGroups,
		"page":         page,
		"page_size":    pageSize,
	})
}

// GetScrapeAnomalies åˆ†é¡µè·å–åˆ®å‰Šå¼‚å¸¸ç»“æœ
func (h *ScanHandler) GetScrapeAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	var total int64
	h.DB.Model(&model.ScrapeAnomaly{}).Count(&total)

	var anomalies []model.ScrapeAnomaly
	offset := (page - 1) * pageSize
	h.DB.Offset(offset).Limit(pageSize).Order("id ASC").Find(&anomalies)

	c.JSON(http.StatusOK, gin.H{
		"data":      anomalies,
		"total":     total,
		"page":      page,
		"page_size": pageSize,
	})
}

// StartEpisodeMappingScan å¯åŠ¨å¼‚å¸¸æ˜ å°„æ‰«æ
func (h *ScanHandler) StartEpisodeMappingScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹å¼‚å¸¸æ˜ å°„æ‰«æ...")
	embyClient, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.ScanEpisodeMappingWithContext(ctx, embyClient, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "æ‰«ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("å¼‚å¸¸æ˜ å°„", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeScrapeAnomalies POST /api/analyze/scrape-anomaly - åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸
func (h *ScanHandler) AnalyzeScrapeAnomalies(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeScrapeAnomaliesFromCache()
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeDuplicateMedia POST /api/analyze/duplicate-media - åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“
func (h *ScanHandler) AnalyzeDuplicateMedia(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeDuplicateMediaFromCache()
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeEpisodeMapping POST /api/analyze/episode-mapping - åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„
func (h *ScanHandler) AnalyzeEpisodeMapping(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	// è·å– TMDB API Key
	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.AnalyzeEpisodeMappingFromCacheWithContext(ctx, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "åˆ†ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	// æŸ¥è¯¢å»é‡åçš„å¼‚å¸¸èŠ‚ç›®æ•°ï¼ˆä¸ç»Ÿè®¡å¡ç‰‡ä¿æŒä¸€è‡´ï¼‰
	var distinctCount int64
	h.DB.Model(&model.EpisodeMappingAnomaly{}).Distinct("emby_item_id").Count(&distinctCount)

	log.Printf("âœ… å¼‚å¸¸æ˜ å°„åˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		result.TotalScanned, distinctCount, result.ErrorCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
		"anomaly_show_count": distinctCount,
	})
}

// CleanupDuplicateMedia POST /api/cleanup/duplicate-media - æ‰¹é‡æ¸…ç†é‡å¤åª’ä½“
// æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å¾…åˆ é™¤ emby_item_id åˆ—è¡¨ï¼Œé€ä¸ªè°ƒç”¨ Emby DeleteVersion æ¥å£
func (h *ScanHandler) CleanupDuplicateMedia(c *gin.Context) {
	var req struct {
		Items []string `json:"items"` // è¦åˆ é™¤çš„ emby_item_id åˆ—è¡¨
	}
	if err := c.ShouldBindJSON(&req); err != nil || len(req.Items) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¡ç›®",
		})
		return
	}

	log.Printf("ğŸ§¹ å¼€å§‹æ‰¹é‡æ¸…ç†é‡å¤åª’ä½“ï¼Œå…± %d ä¸ªæ¡ç›®...", len(req.Items))

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Minute)
	defer cancel()

	deletedCount := 0
	failedCount := 0
	var freedSize int64
	var failedItems []string

	// æŸ¥è¯¢è¿™äº›æ¡ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—å’Œç»Ÿè®¡é‡Šæ”¾ç©ºé—´ï¼‰
	var toDelete []model.DuplicateMedia
	h.DB.Where("emby_item_id IN ?", req.Items).Find(&toDelete)

	// æ„å»º emby_item_id -> DuplicateMedia æ˜ å°„
	itemMap := make(map[string]model.DuplicateMedia)
	for _, d := range toDelete {
		itemMap[d.EmbyItemID] = d
	}

	for _, embyID := range req.Items {
		item, exists := itemMap[embyID]

		// è°ƒç”¨ Emby DeleteVersion æ¥å£
		if err := client.DeleteVersion(ctx, embyID); err != nil {
			log.Printf("âŒ åˆ é™¤ç‰ˆæœ¬å¤±è´¥ [%s]: %v", embyID, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		if exists {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s] %s (%.1f MB)", embyID, item.Path, float64(item.FileSize)/1024/1024)
			freedSize += item.FileSize
		} else {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s]", embyID)
		}
		deletedCount++
	}

	// ä»æ•°æ®åº“ä¸­åˆ é™¤å·²æ¸…ç†çš„è®°å½•
	if deletedCount > 0 {
		// æ’é™¤å¤±è´¥çš„ï¼Œåªåˆ é™¤æˆåŠŸçš„
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.DuplicateMedia{})
		}

		// æ¸…ç†åªå‰©ä¸€æ¡è®°å½•çš„åˆ†ç»„ï¼ˆä¸å†æ˜¯é‡å¤ï¼‰
		h.DB.Exec(`DELETE FROM duplicate_media WHERE group_key IN (
			SELECT group_key FROM duplicate_media GROUP BY group_key HAVING COUNT(*) < 2
		)`)
	}

	log.Printf("âœ… é‡å¤åª’ä½“æ¸…ç†å®Œæˆ: åˆ é™¤ %d ä¸ª, é‡Šæ”¾ %.1f MB, å¤±è´¥ %d ä¸ª",
		deletedCount, float64(freedSize)/1024/1024, failedCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "æ¸…ç†å®Œæˆ",
		"data": gin.H{
			"deleted_count": deletedCount,
			"freed_size":    freedSize,
			"failed_count":  failedCount,
			"failed_items":  failedItems,
		},
	})
}

// PreviewDuplicateCleanup GET /api/cleanup/duplicate-media/preview - é¢„è§ˆå¾…æ¸…ç†çš„é‡å¤åª’ä½“
// è¿”å›æ‰€æœ‰é‡å¤ç»„ï¼Œæ¯ç»„åŒ…å«å…¨éƒ¨æ¡ç›®ï¼Œå¹¶æ ‡è®°å»ºè®®åˆ é™¤çš„ï¼ˆä½“ç§¯è¾ƒå°çš„ï¼‰
func (h *ScanHandler) PreviewDuplicateCleanup(c *gin.Context) {
	// è·å–æ‰€æœ‰é‡å¤åª’ä½“è®°å½•ï¼ŒæŒ‰åˆ†ç»„å’Œæ–‡ä»¶å¤§å°å‡åºæ’åº
	var duplicates []model.DuplicateMedia
	h.DB.Order("group_key ASC, file_size ASC").Find(&duplicates)

	// æŒ‰ group_key åˆ†ç»„
	groups := make(map[string][]model.DuplicateMedia)
	var groupOrder []string
	for _, d := range duplicates {
		if _, exists := groups[d.GroupKey]; !exists {
			groupOrder = append(groupOrder, d.GroupKey)
		}
		groups[d.GroupKey] = append(groups[d.GroupKey], d)
	}

	type previewItem struct {
		EmbyItemID    string `json:"emby_item_id"`
		Name          string `json:"name"`
		Type          string `json:"type"`
		Path          string `json:"path"`
		FileSize      int64  `json:"file_size"`
		ShouldDelete  bool   `json:"should_delete"` // å»ºè®®åˆ é™¤ï¼ˆä½“ç§¯è¾ƒå°çš„ï¼‰
	}

	type previewGroup struct {
		GroupKey  string        `json:"group_key"`
		GroupName string        `json:"group_name"`
		Items     []previewItem `json:"items"`
	}

	var result []previewGroup
	totalDeleteCount := 0
	var totalFreedSize int64

	for _, key := range groupOrder {
		groupItems := groups[key]
		if len(groupItems) < 2 {
			continue
		}

		pg := previewGroup{
			GroupKey:  key,
			GroupName: groupItems[0].GroupName,
		}

		// æŒ‰æ–‡ä»¶å¤§å°å‡åºæ’åˆ—ï¼Œä¿ç•™æœ€åä¸€ä¸ªï¼ˆä½“ç§¯æœ€å¤§çš„ï¼‰ï¼Œå…¶ä½™å»ºè®®åˆ é™¤
		// å¤§å°ç›¸åŒæ—¶é»˜è®¤åˆ é™¤æ’åœ¨å‰é¢çš„
		lastIdx := len(groupItems) - 1
		for i, item := range groupItems {
			shouldDelete := i < lastIdx // æœ€åä¸€ä¸ªä¿ç•™ï¼Œå…¶ä½™åˆ é™¤
			pg.Items = append(pg.Items, previewItem{
				EmbyItemID:   item.EmbyItemID,
				Name:         item.Name,
				Type:         item.Type,
				Path:         item.Path,
				FileSize:     item.FileSize,
				ShouldDelete: shouldDelete,
			})
			if shouldDelete {
				totalDeleteCount++
				totalFreedSize += item.FileSize
			}
		}

		result = append(result, pg)
	}

	c.JSON(http.StatusOK, gin.H{
		"data":               result,
		"total_groups":       len(result),
		"total_delete_count": totalDeleteCount,
		"total_freed_size":   totalFreedSize,
	})
}

// FormatAnalysisSummary æ ¼å¼åŒ–åˆ†æç»“æœæ‘˜è¦æ—¥å¿—å­—ç¬¦ä¸²
func FormatAnalysisSummary(analysisType string, result *service.ScanResult) string {
	return fmt.Sprintf("âœ… %såˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		analysisType, result.TotalScanned, result.AnomalyCount, result.ErrorCount)
}

// CleanupScrapeAnomalies POST /api/cleanup/scrape-anomaly - æ‰¹é‡åˆ é™¤åˆ®å‰Šå¼‚å¸¸æ¡ç›®
// æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å¾…åˆ é™¤ emby_item_id åˆ—è¡¨ï¼Œé€ä¸ªè°ƒç”¨ Emby DeleteItem æ¥å£
func (h *ScanHandler) CleanupScrapeAnomalies(c *gin.Context) {
	var req struct {
		Items []string `json:"items"` // è¦åˆ é™¤çš„ emby_item_id åˆ—è¡¨
	}
	if err := c.ShouldBindJSON(&req); err != nil || len(req.Items) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¡ç›®",
		})
		return
	}

	log.Printf("ğŸ§¹ å¼€å§‹æ‰¹é‡åˆ é™¤åˆ®å‰Šå¼‚å¸¸æ¡ç›®ï¼Œå…± %d ä¸ª...", len(req.Items))

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Minute)
	defer cancel()

	deletedCount := 0
	failedCount := 0
	var failedItems []string

	// æŸ¥è¯¢è¿™äº›æ¡ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
	var toDelete []model.ScrapeAnomaly
	h.DB.Where("emby_item_id IN ?", req.Items).Find(&toDelete)

	// æ„å»º emby_item_id -> ScrapeAnomaly æ˜ å°„
	itemMap := make(map[string]model.ScrapeAnomaly)
	for _, d := range toDelete {
		itemMap[d.EmbyItemID] = d
	}

	for _, embyID := range req.Items {
		item, exists := itemMap[embyID]

		// è°ƒç”¨ Emby DeleteItem æ¥å£
		if err := client.DeleteItem(ctx, embyID); err != nil {
			log.Printf("âŒ åˆ é™¤æ¡ç›®å¤±è´¥ [%s]: %v", embyID, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		if exists {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s] %s", embyID, item.Name)
		} else {
			log.Printf("ğŸ—‘ï¸  å·²åˆ é™¤ [%s]", embyID)
		}
		deletedCount++
	}

	// ä»æ•°æ®åº“ä¸­åˆ é™¤å·²æ¸…ç†çš„è®°å½•
	if deletedCount > 0 {
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.ScrapeAnomaly{})
		}
	}

	// åŒæ—¶æ¸…ç† media_cache ä¸­å¯¹åº”çš„ç¼“å­˜è®°å½•
	if deletedCount > 0 {
		successIDs := make([]string, 0, deletedCount)
		for _, id := range req.Items {
			isFailed := false
			for _, fid := range failedItems {
				if id == fid {
					isFailed = true
					break
				}
			}
			if !isFailed {
				successIDs = append(successIDs, id)
			}
		}
		if len(successIDs) > 0 {
			h.DB.Where("emby_item_id IN ?", successIDs).Delete(&model.MediaCache{})
		}
	}

	log.Printf("âœ… åˆ®å‰Šå¼‚å¸¸æ¸…ç†å®Œæˆ: åˆ é™¤ %d ä¸ª, å¤±è´¥ %d ä¸ª", deletedCount, failedCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "æ¸…ç†å®Œæˆ",
		"data": gin.H{
			"deleted_count": deletedCount,
			"failed_count":  failedCount,
			"failed_items":  failedItems,
		},
	})
}

// BatchFindPosters POST /api/cleanup/batch-find-posters - æ‰¹é‡æŸ¥æ‰¾å¹¶è®¾ç½®å°é¢
// æ¥æ”¶å‰ç«¯ä¼ æ¥çš„å¾…å¤„ç† emby_item_id åˆ—è¡¨ï¼Œè‡ªåŠ¨æŸ¥æ‰¾å¹¶è®¾ç½®ç¬¬ä¸€ä¸ªå¯ç”¨çš„æµ·æŠ¥
func (h *ScanHandler) BatchFindPosters(c *gin.Context) {
	var req struct {
		Items []string `json:"items"` // è¦å¤„ç†çš„ emby_item_id åˆ—è¡¨
	}
	if err := c.ShouldBindJSON(&req); err != nil || len(req.Items) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·é€‰æ‹©è¦å¤„ç†çš„æ¡ç›®",
		})
		return
	}

	log.Printf("ğŸ–¼ï¸  å¼€å§‹æ‰¹é‡æŸ¥æ‰¾å°é¢ï¼Œå…± %d ä¸ª...", len(req.Items))

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 10*time.Minute)
	defer cancel()

	successCount := 0
	failedCount := 0
	noImageCount := 0
	var failedItems []string
	var noImageItems []string

	// æŸ¥è¯¢è¿™äº›æ¡ç›®çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
	var items []model.ScrapeAnomaly
	h.DB.Where("emby_item_id IN ?", req.Items).Find(&items)

	// æ„å»º emby_item_id -> ScrapeAnomaly æ˜ å°„
	itemMap := make(map[string]model.ScrapeAnomaly)
	for _, item := range items {
		itemMap[item.EmbyItemID] = item
	}

	for _, embyID := range req.Items {
		item, exists := itemMap[embyID]
		itemName := embyID
		if exists {
			itemName = item.Name
		}

		// è·å–è¿œç¨‹å›¾ç‰‡åˆ—è¡¨
		remoteImages, err := client.GetRemoteImages(ctx, embyID, "Primary")
		if err != nil {
			log.Printf("âŒ è·å–è¿œç¨‹å›¾ç‰‡å¤±è´¥ [%s] %s: %v", embyID, itemName, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		// æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å›¾ç‰‡
		if len(remoteImages.Images) == 0 {
			log.Printf("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨å°é¢ [%s] %s", embyID, itemName)
			noImageCount++
			noImageItems = append(noImageItems, embyID)
			continue
		}

		// é€‰æ‹©ç¬¬ä¸€ä¸ªå›¾ç‰‡
		firstImage := remoteImages.Images[0]

		// ä¸‹è½½å¹¶è®¾ç½®å°é¢
		err = client.DownloadRemoteImage(ctx, embyID, "Primary", firstImage.URL, firstImage.ProviderName)
		if err != nil {
			log.Printf("âŒ ä¸‹è½½å°é¢å¤±è´¥ [%s] %s: %v", embyID, itemName, err)
			failedCount++
			failedItems = append(failedItems, embyID)
			continue
		}

		log.Printf("âœ… å·²è®¾ç½®å°é¢ [%s] %s (æ¥æº: %s)", embyID, itemName, firstImage.ProviderName)
		successCount++

		// æ›´æ–°æ•°æ®åº“ä¸­çš„ missing_poster æ ‡è®°
		if exists {
			h.DB.Model(&model.ScrapeAnomaly{}).
				Where("emby_item_id = ?", embyID).
				Update("missing_poster", false)
		}

		// æ›´æ–°ç¼“å­˜ä¸­çš„ has_poster æ ‡è®°
		h.DB.Model(&model.MediaCache{}).
			Where("emby_item_id = ?", embyID).
			Update("has_poster", true)
	}

	log.Printf("âœ… æ‰¹é‡æŸ¥æ‰¾å°é¢å®Œæˆ: æˆåŠŸ %d ä¸ª, å¤±è´¥ %d ä¸ª, æ— å¯ç”¨å›¾ç‰‡ %d ä¸ª", successCount, failedCount, noImageCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰¹é‡æŸ¥æ‰¾å°é¢å®Œæˆ",
		"data": gin.H{
			"success_count":   successCount,
			"failed_count":    failedCount,
			"no_image_count":  noImageCount,
			"failed_items":    failedItems,
			"no_image_items":  noImageItems,
		},
	})
}

// FindSinglePoster POST /api/cleanup/find-single-poster - å•ä¸ªæŸ¥æ‰¾å¹¶è®¾ç½®å°é¢
// æ¥æ”¶å•ä¸ª emby_item_idï¼Œè‡ªåŠ¨æŸ¥æ‰¾å¹¶è®¾ç½®ç¬¬ä¸€ä¸ªå¯ç”¨çš„æµ·æŠ¥
func (h *ScanHandler) FindSinglePoster(c *gin.Context) {
	var req struct {
		ItemID string `json:"item_id" binding:"required"` // è¦å¤„ç†çš„ emby_item_id
	}
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·æä¾›æœ‰æ•ˆçš„æ¡ç›®ID",
		})
		return
	}

	log.Printf("ğŸ–¼ï¸  å¼€å§‹æŸ¥æ‰¾å°é¢: %s", req.ItemID)

	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	ctx, cancel := context.WithTimeout(c.Request.Context(), 1*time.Minute)
	defer cancel()

	// æŸ¥è¯¢æ¡ç›®ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
	var item model.ScrapeAnomaly
	h.DB.Where("emby_item_id = ?", req.ItemID).First(&item)
	itemName := req.ItemID
	if item.ID != 0 {
		itemName = item.Name
	}

	// è·å–è¿œç¨‹å›¾ç‰‡åˆ—è¡¨
	remoteImages, err := client.GetRemoteImages(ctx, req.ItemID, "Primary")
	if err != nil {
		log.Printf("âŒ è·å–è¿œç¨‹å›¾ç‰‡å¤±è´¥ [%s] %s: %v", req.ItemID, itemName, err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "è·å–è¿œç¨‹å›¾ç‰‡å¤±è´¥",
			"error":   err.Error(),
		})
		return
	}

	// æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å›¾ç‰‡
	if len(remoteImages.Images) == 0 {
		log.Printf("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨å°é¢ [%s] %s", req.ItemID, itemName)
		c.JSON(http.StatusNotFound, gin.H{
			"code":    404,
			"message": "æœªæ‰¾åˆ°å¯ç”¨çš„å°é¢å›¾ç‰‡",
		})
		return
	}

	// é€‰æ‹©ç¬¬ä¸€ä¸ªå›¾ç‰‡
	firstImage := remoteImages.Images[0]

	// ä¸‹è½½å¹¶è®¾ç½®å°é¢
	err = client.DownloadRemoteImage(ctx, req.ItemID, "Primary", firstImage.URL, firstImage.ProviderName)
	if err != nil {
		log.Printf("âŒ ä¸‹è½½å°é¢å¤±è´¥ [%s] %s: %v", req.ItemID, itemName, err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "ä¸‹è½½å°é¢å¤±è´¥",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("âœ… å·²è®¾ç½®å°é¢ [%s] %s (æ¥æº: %s)", req.ItemID, itemName, firstImage.ProviderName)

	// æ›´æ–°æ•°æ®åº“ä¸­çš„ missing_poster æ ‡è®°
	if item.ID != 0 {
		h.DB.Model(&model.ScrapeAnomaly{}).
			Where("emby_item_id = ?", req.ItemID).
			Update("missing_poster", false)
	}

	// æ›´æ–°ç¼“å­˜ä¸­çš„ has_poster æ ‡è®°
	h.DB.Model(&model.MediaCache{}).
		Where("emby_item_id = ?", req.ItemID).
		Update("has_poster", true)

	c.JSON(http.StatusOK, gin.H{
		"message": "å°é¢è®¾ç½®æˆåŠŸ",
		"data": gin.H{
			"item_id":       req.ItemID,
			"provider_name": firstImage.ProviderName,
		},
	})
}

// GetAnalysisStatus è·å–å„åˆ†ææ¨¡å—çš„æœ€ååˆ†ææ—¶é—´å’Œå¼‚å¸¸æ•°é‡
func (h *ScanHandler) GetAnalysisStatus(c *gin.Context) {
	type moduleStatus struct {
		LastAnalyzedAt *time.Time `json:"last_analyzed_at"`
		AnomalyCount   int64     `json:"anomaly_count"`
	}

	status := make(map[string]moduleStatus)

	modules := []struct {
		key   string
		model interface{}
		countDistinct string // å¦‚æœéœ€è¦ distinct è®¡æ•°
	}{
		{"scrape_anomaly", &model.ScrapeAnomaly{}, ""},
		{"duplicate_media", &model.DuplicateMedia{}, "group_key"},
		{"episode_mapping", &model.EpisodeMappingAnomaly{}, "emby_item_id"},
	}

	for _, m := range modules {
		var count int64
		if m.countDistinct != "" {
			h.DB.Model(m.model).Distinct(m.countDistinct).Count(&count)
		} else {
			h.DB.Model(m.model).Count(&count)
		}

		// ä» scan_logs è·å–æœ€åæ‰§è¡Œæ—¶é—´
		var lastLog model.ScanLog
		var lastTime *time.Time
		if err := h.DB.Where("module = ?", m.key).Order("finished_at DESC").First(&lastLog).Error; err == nil {
			lastTime = &lastLog.FinishedAt
		}

		status[m.key] = moduleStatus{LastAnalyzedAt: lastTime, AnomalyCount: count}
	}

	c.JSON(http.StatusOK, gin.H{
		"data": status,
	})
}

// EpisodeMappingGroup æŒ‰èŠ‚ç›®èšåˆçš„å¼‚å¸¸æ˜ å°„ç»„
type EpisodeMappingGroup struct {
	EmbyItemID   string                        `json:"emby_item_id"`
	Name         string                        `json:"name"`
	TmdbID       int                           `json:"tmdb_id"`
	SeasonCount  int                           `json:"season_count"` // å¼‚å¸¸å­£æ•°é‡
	Seasons      []model.EpisodeMappingAnomaly `json:"seasons"`
}

// GetEpisodeMappingAnomalies åˆ†é¡µè·å–å¼‚å¸¸æ˜ å°„ç»“æœï¼ˆæŒ‰èŠ‚ç›®èšåˆï¼‰
// æ”¯æŒå‚æ•°: page, pageSize, search(åç§°æœç´¢), sort(æ’åºå­—æ®µ), filter(single/multi)
func (h *ScanHandler) GetEpisodeMappingAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))
	search := c.DefaultQuery("search", "")
	sortBy := c.DefaultQuery("sort", "season_count_desc") // é»˜è®¤æŒ‰å¼‚å¸¸å­£æ•°é‡é™åº
	filter := c.DefaultQuery("filter", "")                 // single / multi / ç©º

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// æ„å»ºåŸºç¡€æŸ¥è¯¢
	baseQuery := h.DB.Model(&model.EpisodeMappingAnomaly{})

	// æœç´¢æ¡ä»¶
	if search != "" {
		baseQuery = baseQuery.Where("name LIKE ?", "%"+search+"%")
	}

	// æ„å»ºåˆ†ç»„å­æŸ¥è¯¢ï¼ˆç”¨äºç­›é€‰å’Œæ’åºï¼‰
	// å…ˆè·å–æ¯ä¸ª emby_item_id çš„å¼‚å¸¸å­£æ•°é‡
	groupQuery := baseQuery.Session(&gorm.Session{NewDB: true}).
		Model(&model.EpisodeMappingAnomaly{})
	if search != "" {
		groupQuery = groupQuery.Where("name LIKE ?", "%"+search+"%")
	}

	// ç­›é€‰æ¡ä»¶
	switch filter {
	case "single":
		// åªæœ‰1å­£å¼‚å¸¸çš„
		groupQuery = groupQuery.Select("emby_item_id, COUNT(*) as season_count").
			Group("emby_item_id").
			Having("COUNT(*) = 1")
	case "multi":
		// å¤šå­£å¼‚å¸¸ï¼šæœ¬åœ°æœ‰å¤šå­£ï¼Œä½† TMDB åªæœ‰ä¸€å­£
		groupQuery = groupQuery.Select("emby_item_id, COUNT(*) as season_count").
			Where("local_season_count > 1 AND tmdb_season_count = 1").
			Group("emby_item_id")
	default:
		groupQuery = groupQuery.Select("emby_item_id, COUNT(*) as season_count").
			Group("emby_item_id")
	}

	// è®¡ç®—æ€»æ•°
	type countResult struct {
		Total int64
	}
	var totalCount int64
	countQuery := h.DB.Table("(?) as sub", groupQuery).Count(&totalCount)
	_ = countQuery

	// æ’åº
	var orderClause string
	switch sortBy {
	case "season_count_desc":
		orderClause = "season_count DESC, MIN(name) ASC"
	case "season_count_asc":
		orderClause = "season_count ASC, MIN(name) ASC"
	case "name_asc":
		orderClause = "MIN(name) ASC"
	case "name_desc":
		orderClause = "MIN(name) DESC"
	default:
		orderClause = "season_count DESC, MIN(name) ASC"
	}

	// åˆ†é¡µè·å–èŠ‚ç›® ID åˆ—è¡¨
	type groupRow struct {
		EmbyItemID  string `gorm:"column:emby_item_id"`
		SeasonCount int    `gorm:"column:season_count"`
	}
	var groupRows []groupRow
	offset := (page - 1) * pageSize

	idQuery := h.DB.Model(&model.EpisodeMappingAnomaly{})
	if search != "" {
		idQuery = idQuery.Where("name LIKE ?", "%"+search+"%")
	}
	idQuery = idQuery.Select("emby_item_id, COUNT(*) as season_count").
		Group("emby_item_id")

	switch filter {
	case "single":
		idQuery = idQuery.Having("COUNT(*) = 1")
	case "multi":
		idQuery = idQuery.Where("local_season_count > 1 AND tmdb_season_count = 1")
	}

	idQuery.Order(orderClause).
		Offset(offset).Limit(pageSize).
		Find(&groupRows)

	embyItemIDs := make([]string, len(groupRows))
	seasonCountMap := make(map[string]int)
	for i, r := range groupRows {
		embyItemIDs[i] = r.EmbyItemID
		seasonCountMap[r.EmbyItemID] = r.SeasonCount
	}

	var groups []EpisodeMappingGroup
	if len(embyItemIDs) > 0 {
		// è·å–å¼‚å¸¸è®°å½•ï¼ˆç”¨äºæ˜¾ç¤ºå·®å¼‚ï¼‰
		var anomalies []model.EpisodeMappingAnomaly
		h.DB.Where("emby_item_id IN ?", embyItemIDs).
			Order("season_number ASC").
			Find(&anomalies)

		// è·å–æ‰€æœ‰å­£çš„å®Œæ•´ä¿¡æ¯ï¼ˆä» season_cacheï¼‰
		var seasonCaches []model.SeasonCache
		h.DB.Where("series_emby_item_id IN ?", embyItemIDs).
			Order("season_number ASC").
			Find(&seasonCaches)

		// æ„å»º emby_item_id -> å­£ä¿¡æ¯çš„æ˜ å°„
		seasonMap := make(map[string]map[int]model.SeasonCache) // emby_item_id -> season_number -> SeasonCache
		for _, sc := range seasonCaches {
			if _, exists := seasonMap[sc.SeriesEmbyItemID]; !exists {
				seasonMap[sc.SeriesEmbyItemID] = make(map[int]model.SeasonCache)
			}
			seasonMap[sc.SeriesEmbyItemID][sc.SeasonNumber] = sc
		}

		// æ„å»º emby_item_id -> å¼‚å¸¸è®°å½•çš„æ˜ å°„
		anomalyMap := make(map[string]map[int]model.EpisodeMappingAnomaly) // emby_item_id -> season_number -> Anomaly
		seriesInfoMap := make(map[string]model.EpisodeMappingAnomaly)      // emby_item_id -> å‰§é›†ä¿¡æ¯ï¼ˆå–ç¬¬ä¸€æ¡ï¼‰
		for _, a := range anomalies {
			if _, exists := seriesInfoMap[a.EmbyItemID]; !exists {
				seriesInfoMap[a.EmbyItemID] = a
			}
			if _, exists := anomalyMap[a.EmbyItemID]; !exists {
				anomalyMap[a.EmbyItemID] = make(map[int]model.EpisodeMappingAnomaly)
			}
			anomalyMap[a.EmbyItemID][a.SeasonNumber] = a
		}

		// è·å– TMDB ç¼“å­˜æ•°æ®
		tmdbIDs := make([]int, 0)
		for _, a := range seriesInfoMap {
			tmdbIDs = append(tmdbIDs, a.TmdbID)
		}
		var tmdbCaches []model.TmdbCache
		if len(tmdbIDs) > 0 {
			h.DB.Where("tmdb_id IN ?", tmdbIDs).Find(&tmdbCaches)
		}

		// æ„å»º tmdb_id -> season_number -> episode_count çš„æ˜ å°„
		tmdbMap := make(map[int]map[int]int) // tmdb_id -> season_number -> episode_count
		for _, tc := range tmdbCaches {
			if _, exists := tmdbMap[tc.TmdbID]; !exists {
				tmdbMap[tc.TmdbID] = make(map[int]int)
			}
			tmdbMap[tc.TmdbID][tc.SeasonNumber] = tc.EpisodeCount
		}

		// æ„å»ºè¿”å›ç»“æœ
		groupMap := make(map[string]*EpisodeMappingGroup)
		for embyItemID, seasons := range seasonMap {
			info, hasInfo := seriesInfoMap[embyItemID]
			if !hasInfo {
				continue // æ²¡æœ‰å¼‚å¸¸è®°å½•ï¼Œè·³è¿‡ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
			}

			g := &EpisodeMappingGroup{
				EmbyItemID:  embyItemID,
				Name:        info.Name,
				TmdbID:      info.TmdbID,
				SeasonCount: seasonCountMap[embyItemID],
				Seasons:     []model.EpisodeMappingAnomaly{},
			}

			// éå†æ‰€æœ‰æœ¬åœ°å­£ï¼Œæ„å»ºå®Œæ•´çš„å­£ä¿¡æ¯
			// å…ˆæ”¶é›†æ‰€æœ‰å­£å·å¹¶æ’åº
			seasonNumbers := make([]int, 0, len(seasons))
			for seasonNum := range seasons {
				if seasonNum > 0 { // è·³è¿‡ç‰¹åˆ«ç¯‡
					seasonNumbers = append(seasonNumbers, seasonNum)
				}
			}
			// æ’åºå­£å·
			sort.Ints(seasonNumbers)

			// æŒ‰æ’åºåçš„å­£å·æ„å»ºå­£è®°å½•
			for _, seasonNum := range seasonNumbers {
				sc := seasons[seasonNum]

				// è·å– TMDB é›†æ•°
				tmdbEpisodes := 0
				if tmdbSeasons, exists := tmdbMap[info.TmdbID]; exists {
					if count, ok := tmdbSeasons[seasonNum]; ok {
						tmdbEpisodes = count
					}
				}

				// è®¡ç®—å·®å¼‚
				diff := sc.EpisodeCount - tmdbEpisodes
				if diff < 0 {
					diff = -diff
				}

				// æ„å»ºå­£è®°å½•
				seasonRecord := model.EpisodeMappingAnomaly{
					EmbyItemID:       embyItemID,
					Name:             info.Name,
					TmdbID:           info.TmdbID,
					SeasonNumber:     seasonNum,
					LocalEpisodes:    sc.EpisodeCount,
					TmdbEpisodes:     tmdbEpisodes,
					Difference:       diff,
					LocalSeasonCount: info.LocalSeasonCount,
					TmdbSeasonCount:  info.TmdbSeasonCount,
				}

				g.Seasons = append(g.Seasons, seasonRecord)
			}

			groupMap[embyItemID] = g
		}

		// æŒ‰ groupRows çš„é¡ºåºè¾“å‡ºï¼ˆä¿æŒæ’åºï¼‰
		for _, r := range groupRows {
			if g, ok := groupMap[r.EmbyItemID]; ok {
				groups = append(groups, *g)
			}
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"data":      groups,
		"total":     totalCount,
		"page":      page,
		"page_size": pageSize,
	})
}
