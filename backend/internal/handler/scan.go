package handler

import (
	"context"
	"fmt"
	"log"
	"net/http"
	"strconv"
	"time"

	"embyforge/internal/emby"
	"embyforge/internal/model"
	"embyforge/internal/service"
	"embyforge/internal/tmdb"

	"github.com/gin-gonic/gin"
	"gorm.io/gorm"
)

// defaultScanTimeout æ‰«ææ“ä½œé»˜è®¤è¶…æ—¶æ—¶é—´
const defaultScanTimeout = 30 * time.Minute

// ScanHandler æ‰«æå¤„ç†å™¨
type ScanHandler struct {
	DB          *gorm.DB
	ScanService *service.ScanService
}

// NewScanHandler åˆ›å»ºæ‰«æå¤„ç†å™¨
func NewScanHandler(db *gorm.DB) *ScanHandler {
	return &ScanHandler{
		DB:          db,
		ScanService: service.NewScanService(db),
	}
}

// getTMDBAPIKey ä»æ•°æ®åº“è¯»å– TMDB API Key
func (h *ScanHandler) getTMDBAPIKey() (string, error) {
	var config model.SystemConfig
	if err := h.DB.Where("key = ?", "tmdb_api_key").First(&config).Error; err != nil {
		return "", err
	}
	return config.Value, nil
}

// getEmbyClient ä»æ•°æ®åº“è·å– Emby é…ç½®å¹¶åˆ›å»ºå®¢æˆ·ç«¯
func (h *ScanHandler) getEmbyClient() (*emby.Client, error) {
	var config model.EmbyConfig
	if err := h.DB.First(&config).Error; err != nil {
		return nil, err
	}
	return emby.NewClient(config.Host, config.Port, config.APIKey), nil
}

// StartScrapeAnomalyScan å¯åŠ¨åˆ®å‰Šå¼‚å¸¸æ‰«æ
func (h *ScanHandler) StartScrapeAnomalyScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åˆ®å‰Šå¼‚å¸¸æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanScrapeAnomalies(client)
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// StartDuplicateMediaScan å¯åŠ¨é‡å¤åª’ä½“æ‰«æ
func (h *ScanHandler) StartDuplicateMediaScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹é‡å¤åª’ä½“æ‰«æ...")
	client, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	result, err := h.ScanService.ScanDuplicateMedia(client)
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// GetDuplicateMedia åˆ†é¡µè·å–é‡å¤åª’ä½“ç»“æœï¼ˆæŒ‰ GroupKey åˆ†ç»„ï¼‰
func (h *ScanHandler) GetDuplicateMedia(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// è·å–ä¸åŒåˆ†ç»„çš„æ€»æ•°
	var totalGroups int64
	h.DB.Model(&model.DuplicateMedia{}).Distinct("group_key").Count(&totalGroups)

	// åˆ†é¡µè·å–åˆ†ç»„é”®å’Œåˆ†ç»„å
	type groupInfo struct {
		GroupKey  string `json:"group_key"`
		GroupName string `json:"group_name"`
		Count     int64  `json:"count"`
	}
	var groups []groupInfo
	offset := (page - 1) * pageSize
	h.DB.Model(&model.DuplicateMedia{}).
		Select("group_key, MAX(group_name) as group_name, COUNT(*) as count").
		Group("group_key").
		Order("count DESC, group_key ASC").
		Offset(offset).
		Limit(pageSize).
		Find(&groups)

	// è·å–è¿™äº›åˆ†ç»„ä¸‹çš„æ‰€æœ‰è®°å½•
	groupKeys := make([]string, len(groups))
	for i, g := range groups {
		groupKeys[i] = g.GroupKey
	}

	var duplicates []model.DuplicateMedia
	if len(groupKeys) > 0 {
		h.DB.Where("group_key IN ?", groupKeys).Order("group_key ASC, type ASC, name ASC").Find(&duplicates)
	}

	// æŒ‰ GroupKey åˆ†ç»„è¿”å›ï¼ŒåŒ…å«åˆ†ç»„ä¿¡æ¯
	type groupResult struct {
		GroupKey  string                 `json:"group_key"`
		GroupName string                 `json:"group_name"`
		Count     int64                  `json:"count"`
		Items     []model.DuplicateMedia `json:"items"`
	}

	// æ„å»ºåˆ†ç»„æ˜ å°„
	itemsByKey := make(map[string][]model.DuplicateMedia)
	for _, d := range duplicates {
		itemsByKey[d.GroupKey] = append(itemsByKey[d.GroupKey], d)
	}

	var results []groupResult
	for _, g := range groups {
		results = append(results, groupResult{
			GroupKey:  g.GroupKey,
			GroupName: g.GroupName,
			Count:     g.Count,
			Items:     itemsByKey[g.GroupKey],
		})
	}

	c.JSON(http.StatusOK, gin.H{
		"data":         results,
		"total_groups": totalGroups,
		"page":         page,
		"page_size":    pageSize,
	})
}

// GetScrapeAnomalies åˆ†é¡µè·å–åˆ®å‰Šå¼‚å¸¸ç»“æœ
func (h *ScanHandler) GetScrapeAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	var total int64
	h.DB.Model(&model.ScrapeAnomaly{}).Count(&total)

	var anomalies []model.ScrapeAnomaly
	offset := (page - 1) * pageSize
	h.DB.Offset(offset).Limit(pageSize).Order("id ASC").Find(&anomalies)

	c.JSON(http.StatusOK, gin.H{
		"data":      anomalies,
		"total":     total,
		"page":      page,
		"page_size": pageSize,
	})
}

// StartEpisodeMappingScan å¯åŠ¨å¼‚å¸¸æ˜ å°„æ‰«æ
func (h *ScanHandler) StartEpisodeMappingScan(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹å¼‚å¸¸æ˜ å°„æ‰«æ...")
	embyClient, err := h.getEmbyClient()
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·å…ˆé…ç½® Emby æœåŠ¡å™¨è¿æ¥ä¿¡æ¯",
		})
		return
	}

	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.ScanEpisodeMappingWithContext(ctx, embyClient, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "æ‰«ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„æ‰«æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
			"data":    result,
		})
		return
	}

	log.Printf("%s", service.FormatScanSummary("å¼‚å¸¸æ˜ å°„", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "æ‰«æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeScrapeAnomalies POST /api/analyze/scrape-anomaly - åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸
func (h *ScanHandler) AnalyzeScrapeAnomalies(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æåˆ®å‰Šå¼‚å¸¸...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeScrapeAnomaliesFromCache()
	if err != nil {
		log.Printf("âš ï¸ åˆ®å‰Šå¼‚å¸¸åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("åˆ®å‰Šå¼‚å¸¸", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeDuplicateMedia POST /api/analyze/duplicate-media - åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“
func (h *ScanHandler) AnalyzeDuplicateMedia(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æé‡å¤åª’ä½“...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	result, err := h.ScanService.AnalyzeDuplicateMediaFromCache()
	if err != nil {
		log.Printf("âš ï¸ é‡å¤åª’ä½“åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	log.Printf("%s", FormatAnalysisSummary("é‡å¤åª’ä½“", result))
	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
	})
}

// AnalyzeEpisodeMapping POST /api/analyze/episode-mapping - åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„
func (h *ScanHandler) AnalyzeEpisodeMapping(c *gin.Context) {
	log.Printf("ğŸ” å¼€å§‹åŸºäºç¼“å­˜åˆ†æå¼‚å¸¸æ˜ å°„...")

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
	var count int64
	h.DB.Model(&model.MediaCache{}).Count(&count)
	if count == 0 {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆåˆ°æ‰«æåª’ä½“é¡µé¢åŒæ­¥åª’ä½“åº“",
		})
		return
	}

	// è·å– TMDB API Key
	tmdbAPIKey, err := h.getTMDBAPIKey()
	if err != nil || tmdbAPIKey == "" {
		c.JSON(http.StatusBadRequest, gin.H{
			"code":    400,
			"message": "è¯·åœ¨ç³»ç»Ÿé…ç½®é¡µé¢é…ç½® TMDB API Key",
		})
		return
	}

	tmdbClient := tmdb.NewClient(tmdbAPIKey)

	ctx, cancel := context.WithTimeout(c.Request.Context(), defaultScanTimeout)
	defer cancel()

	result, err := h.ScanService.AnalyzeEpisodeMappingFromCacheWithContext(ctx, tmdbClient)
	if err != nil {
		if ctx.Err() == context.DeadlineExceeded {
			log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æè¶…æ—¶")
			c.JSON(http.StatusGatewayTimeout, gin.H{
				"code":    504,
				"message": "åˆ†ææ“ä½œè¶…æ—¶",
				"error":   err.Error(),
			})
			return
		}
		log.Printf("âš ï¸ å¼‚å¸¸æ˜ å°„åˆ†æå‡ºé”™: %v", err)
		c.JSON(http.StatusInternalServerError, gin.H{
			"code":    500,
			"message": "åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™",
			"error":   err.Error(),
		})
		return
	}

	// æŸ¥è¯¢å»é‡åçš„å¼‚å¸¸èŠ‚ç›®æ•°ï¼ˆä¸ç»Ÿè®¡å¡ç‰‡ä¿æŒä¸€è‡´ï¼‰
	var distinctCount int64
	h.DB.Model(&model.EpisodeMappingAnomaly{}).Distinct("emby_item_id").Count(&distinctCount)

	log.Printf("âœ… å¼‚å¸¸æ˜ å°„åˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		result.TotalScanned, distinctCount, result.ErrorCount)

	c.JSON(http.StatusOK, gin.H{
		"message": "åˆ†æå®Œæˆ",
		"data":    result,
		"anomaly_show_count": distinctCount,
	})
}

// FormatAnalysisSummary æ ¼å¼åŒ–åˆ†æç»“æœæ‘˜è¦æ—¥å¿—å­—ç¬¦ä¸²
func FormatAnalysisSummary(analysisType string, result *service.ScanResult) string {
	return fmt.Sprintf("âœ… %såˆ†æå®Œæˆ: å…±åˆ†æ %d ä¸ªæ¡ç›®, å‘ç° %d ä¸ªå¼‚å¸¸, %d ä¸ªé”™è¯¯",
		analysisType, result.TotalScanned, result.AnomalyCount, result.ErrorCount)
}

// GetAnalysisStatus è·å–å„åˆ†ææ¨¡å—çš„æœ€ååˆ†ææ—¶é—´å’Œå¼‚å¸¸æ•°é‡
func (h *ScanHandler) GetAnalysisStatus(c *gin.Context) {
	type moduleStatus struct {
		LastAnalyzedAt *time.Time `json:"last_analyzed_at"`
		AnomalyCount   int64     `json:"anomaly_count"`
	}

	status := make(map[string]moduleStatus)

	modules := []struct {
		key   string
		model interface{}
		countDistinct string // å¦‚æœéœ€è¦ distinct è®¡æ•°
	}{
		{"scrape_anomaly", &model.ScrapeAnomaly{}, ""},
		{"duplicate_media", &model.DuplicateMedia{}, "group_key"},
		{"episode_mapping", &model.EpisodeMappingAnomaly{}, "emby_item_id"},
	}

	for _, m := range modules {
		var count int64
		if m.countDistinct != "" {
			h.DB.Model(m.model).Distinct(m.countDistinct).Count(&count)
		} else {
			h.DB.Model(m.model).Count(&count)
		}

		// ä» scan_logs è·å–æœ€åæ‰§è¡Œæ—¶é—´
		var lastLog model.ScanLog
		var lastTime *time.Time
		if err := h.DB.Where("module = ?", m.key).Order("finished_at DESC").First(&lastLog).Error; err == nil {
			lastTime = &lastLog.FinishedAt
		}

		status[m.key] = moduleStatus{LastAnalyzedAt: lastTime, AnomalyCount: count}
	}

	c.JSON(http.StatusOK, gin.H{
		"data": status,
	})
}

// EpisodeMappingGroup æŒ‰èŠ‚ç›®èšåˆçš„å¼‚å¸¸æ˜ å°„ç»„
type EpisodeMappingGroup struct {
	EmbyItemID string                        `json:"emby_item_id"`
	Name       string                        `json:"name"`
	TmdbID     int                           `json:"tmdb_id"`
	Seasons    []model.EpisodeMappingAnomaly `json:"seasons"`
}

// GetEpisodeMappingAnomalies åˆ†é¡µè·å–å¼‚å¸¸æ˜ å°„ç»“æœï¼ˆæŒ‰èŠ‚ç›®èšåˆï¼‰
func (h *ScanHandler) GetEpisodeMappingAnomalies(c *gin.Context) {
	page, _ := strconv.Atoi(c.DefaultQuery("page", "1"))
	pageSize, _ := strconv.Atoi(c.DefaultQuery("pageSize", "20"))

	if page < 1 {
		page = 1
	}
	if pageSize < 1 || pageSize > 100 {
		pageSize = 20
	}

	// æŒ‰ emby_item_id åˆ†ç»„è®¡ç®—æ€»èŠ‚ç›®æ•°
	var total int64
	h.DB.Model(&model.EpisodeMappingAnomaly{}).Distinct("emby_item_id").Count(&total)

	// åˆ†é¡µè·å–å½“å‰é¡µçš„èŠ‚ç›® ID åˆ—è¡¨ï¼ˆæŒ‰åç§°æ’åºï¼‰
	var embyItemIDs []string
	offset := (page - 1) * pageSize
	h.DB.Model(&model.EpisodeMappingAnomaly{}).
		Select("emby_item_id").
		Group("emby_item_id").
		Order("MIN(name) ASC").
		Offset(offset).Limit(pageSize).
		Pluck("emby_item_id", &embyItemIDs)

	var groups []EpisodeMappingGroup
	if len(embyItemIDs) > 0 {
		// è·å–è¿™äº›èŠ‚ç›®çš„æ‰€æœ‰å¼‚å¸¸å­£æ•°æ®ï¼ŒæŒ‰å­£å·æ’åº
		var anomalies []model.EpisodeMappingAnomaly
		h.DB.Where("emby_item_id IN ?", embyItemIDs).
			Order("season_number ASC").
			Find(&anomalies)

		// æŒ‰ emby_item_id èšåˆ
		groupMap := make(map[string]*EpisodeMappingGroup)
		groupOrder := make([]string, 0) // ä¿æŒé¡ºåº
		for _, a := range anomalies {
			g, exists := groupMap[a.EmbyItemID]
			if !exists {
				g = &EpisodeMappingGroup{
					EmbyItemID: a.EmbyItemID,
					Name:       a.Name,
					TmdbID:     a.TmdbID,
					Seasons:    []model.EpisodeMappingAnomaly{},
				}
				groupMap[a.EmbyItemID] = g
				groupOrder = append(groupOrder, a.EmbyItemID)
			}
			g.Seasons = append(g.Seasons, a)
		}

		// æŒ‰åŸå§‹é¡ºåºè¾“å‡º
		for _, id := range groupOrder {
			groups = append(groups, *groupMap[id])
		}
	}

	c.JSON(http.StatusOK, gin.H{
		"data":      groups,
		"total":     total,
		"page":      page,
		"page_size": pageSize,
	})
}
